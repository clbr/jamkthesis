Optimizing Radeon VRAM behavior
===============================

[glossary]
Glossary
--------

[glossary]
AI::
	Artificial Intelligence.

buffer::
	A block of memory used for some purpose. For example, a texture.

Catalyst::
	AMD's proprietary graphics driver.

FPS::
	Frames per second, a common performance metric.

Frame time::
	The inverse of FPS, ie. (milli)seconds per frame.

Linux::
	An open-source operating system

mipmapping::
	A technique of accessing pre-scaled versions of a texture.

r600g::
	The open-source Gallium3D graphics driver responsible for Radeon HD 2000 - HD 6000 
	models.

RAM::
	Random Access Memory, a type of computer memory.

texture::
	A buffer consisting of image data.

VRAM::
	Video RAM, fast on-board memory included on graphics cards.

Introduction
------------

Graphics cards carry some fast, dedicated memory (VRAM). This memory is used to speed up 
tasks such as rendering or parallel computations, by avoiding the latency in having the 
graphics card access the system memory.

VRAM is a limited resource, and as such, all workloads may not fit in a given graphics 
card. In a situation like that, the strategy that decides what parts of the working set to 
keep in VRAM, and which parts can be kept in regular system RAM, is of utmost importance.

If the application needs to access a specific resource in system RAM, it will have a 
corresponding hit in its performance. The VRAM placement strategy should try to rank the 
application's buffers in such a way to maximize the application's performance.

Application performance is commonly measured in two ways: throughput (FPS, frames per 
second) and smoothness (how stable the FPS is). For a good user experience, they both 
must be above some user-dependent threshold.

There are no generally accepted guidelines, as the minimum for an acceptable experience is 
quite subjective. However, many people place the line at 60 FPS with a minimum FPS of 55.
(Minimum playable FPS, 2012)

Both throughput and smoothness must be above the user's minimum requirements, one alone will 
not suffice. For example, a game running at a healthy throughput of 60 FPS, but dipping to 
10 FPS once per second will be extremely irritating, perceived as stalling or stuttering. 
Vice versa, a smooth FPS of 20 will not stutter, but the throughput is not high enough for 
an enjoyable experience, as the user will notice individual images instead of the illusion 
of movement.

Problem background
------------------

The open source graphics drivers on Linux are generally thought of as lower performance, but 
higher stability, than the proprietary drivers provided by the graphics card companies. This 
is a result of the bar for quality in public work; each new version of a proprietary driver 
generally includes many application-specific changes (Catalyst 13.1 release notes, 2013), 
which would not fly in an open-source driver.

Recent advances in optimization have brought the r600g driver close to competing with the 
proprietary Catalyst driver (Larabel 2013a). Depending on the benchmark and hardware, it 
is within 60% - 110% of the Catalyst driver, generally around 80%.

One of the remaining areas yet to be optimized is the VRAM placing strategy. In this 
thesis, research will be done on improving this area of the r600g driver.

There are some practical constraints that the solution will need to abide by. First, there 
is no central location with a holistic view of everything that happens on the graphics card. 
Secondly, the run-time performance of the strategy itself must be adequate. It may have to 
make thousands of decisions per second; having performance go down because the computer is 
thinking on how to improve performance would be counterproductive.

A perfect solution would know everything that goes on in the system. It could predict 
exactly what the application will do next, and in what way does it access each buffer. 
Sadly, either of those is a luxury not available in the real world.

While recording the way the application accesses each buffer is possible in theory, this 
would incur more overhead than could be gained. Thus, the strategy is limited to considering 
worst-case possibilities, and has to make decisions as if the application would use each 
buffer fully. In practice, many buffers are not fully read or written, but only parts of 
them are used, for example due to mipmapping.

The current strategy used in r600g is a simple LRU (least recently used) list. While simple, 
it can lead to quite a bad user experience in non-trivial cases that exceed the VRAM size. A 
buffer that is needed next could have been swapped out to system RAM, dropping performance. 
In the worst case, this creates a ping-pong effect where buffers are constantly moved back 
and forth, dropping performance to single digits FPS. (Larabel 2012a; Very low FPS when 
video memory is full (GART & ram <-> vram swapping) 2013)

Given that the absolute knowledge is not available, the system will have to make do with 
what is there. For example, the system can keep accurate statistics on the times the 
application declares it will read from a buffer. Though the system cannot know how much the 
application will read, it does know the application will read from the buffer. Using this 
information, it can make a worst-case estimation that the application will read the buffer 
in full.

Such worst-case estimation will directly improve the minimum FPS; in other words, the 
smoothness. This has an indirect benefit to performance, but optimal performance will not be 
reached by these means.

The target of this work is to improve on the current strategy, particularly in the cases 
that are currently pathological. The improved system may not gain significantly in 
throughput, but it should gain measurably in smoothness.

Problem definition
------------------

Knowing what information the system has available, and with the goals in mind, the problem 
could be defined more clearly.

The component that decides which buffer to place where is separate from the component that 
has access to accurate statistics. Moving information between these components, the kernel 
and the userspace, is expensive; therefore the information moved must be minimized.

According to the privilege separation, userspace should not be able to directly decide a 
placement in memory space, as this could cause security holes. Moving the entire statistics 
data over would be too expensive. How about ranking the buffers, giving each buffer an 
importance score?

This limits the information needed to pass to the kernel to one integer per buffer. In 
addition, the kernel is free to disregard this hint, keeping in line with the separation. 
Having the relative importance of each buffer known, the kernel should be able to make much 
better buffer placing decisions than the current LRU strategy.

While giving each buffer a score could certainly be done by the usual programming 
techniques, linear/weight calculations and a set of if-conditions, it is believed that the 
relationship of the statistics to the buffer's importance is both non-linear and hard to 
model manually.

Given this assumption, it is likely that such a manual method would not do well in many 
cases, and it would be constantly tweaked to accommodate newly discovered pathological cases. 
There is no existing model for a buffer's importance, and no single right answer to the 
question "how important is this buffer?".

The mainstream solution to solving non-linear, unknown models like that (also known as 
modeling or regression problems) is to use AI solutions such as neural networks.

AI theory
---------

There are many approaches to artificial intelligence. What is common to all of them is the 
ability to make decisions the computer was not told explicitly how to do. They differ in 
their areas of usability, theoretic foundations, whether they are based on real biological 
phenomena, and other ways.

One such approach is the neural network. Invented in the 60's, it mimicks the 
biological brain cells. Such networks are able to generalize, to learn either independently 
or with guidance, and tend to achieve quite decent results. Neural networks have been 
applied to problems such as Backgammon (Tesauro 1994), business data mining (Bigus 1996), 
and text compression (Mahoney 1996) with successful results.

Fragmentation
-------------

Training the network
--------------------

Cost model
~~~~~~~~~~

Activation function
~~~~~~~~~~~~~~~~~~~

Results
-------

Discussion
----------

[bibliography]
Bibliography
------------

Bigus, J. 1996. Data Mining with Neural Networks. 1st ed. Indiana: McGraw-Hill.

Catalyst 13.1 release notes. 2013. AMD Knowledge Base. Retrieved on Jan 15 2013.
http://support.amd.com/en-us/kb-articles/Pages/AMDCatalystSoftwareSuiteVersion131.aspx

Larabel, M. 2012a. Ubuntu 12.10: Open-Source Radeon vs. AMD Catalyst Performance. Phoronix. 
Retrieved on Jan 15 2013.
http://www.phoronix.com/scan.php?page=article&item=ubuntu_1210_amdstock&num=3

Larabel, M. 2013a. AMD's Radeon Gallium3D Starts Posing A Threat To Catalyst. Phoronix. 
Retrieved on Jan 15 2013.
http://www.phoronix.com/scan.php?page=article&item=amd_catalyst_gallium80

Mahoney, M. 1996. Fast text compression with neural networks. Proceedings of the Thirteenth 
International Florida Artificial Intelligence Research Society Conference.

Minimum playable FPS. 2012. Whirlpool forums. Retrieved on Jan 15 2013. 
http://forums.whirlpool.net.au/archive/1890684

Tesauro, G. 1994. TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play. 
Neural Computation 6, 2 (March 1994)

Very low FPS when video memory is full (GART & ram <-> vram swapping). 2013. FreeDesktop.org 
Bugzilla. Retrieved on Jan 15 2013.
https://bugs.freedesktop.org/show_bug.cgi?id=66632
